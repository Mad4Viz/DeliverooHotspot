{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2ef5ed",
   "metadata": {},
   "source": [
    "# Google Places API Data Collection\n",
    "This notebook collects data about food establishments in London using the Google Places API.\n",
    "\n",
    "## Setup Requirements\n",
    "- Google Maps API key in `.env` file\n",
    "- Required libraries: python-dotenv, googlemaps, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc52b70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import googlemaps\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Optional, Dict, List\n",
    "import sys\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='places_api.log'\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1667a1c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 2: Define PlacesDataCollector Class for Deliveroo Driver Focus\n",
    "class PlacesDataCollector:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the PlacesDataCollector with Google Maps client.\"\"\"\n",
    "        self.api_key = os.getenv('GOOGLE_MAPS_API_KEY')\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Google Maps API key not found in environment variables\")\n",
    "        \n",
    "        self.gmaps = googlemaps.Client(key=self.api_key)\n",
    "        self.place_types = [\n",
    "            'restaurant',           \n",
    "            'meal_takeaway',        \n",
    "            'fast_food_restaurant', \n",
    "            'food',                \n",
    "            'meal_delivery',        \n",
    "            'cafe',\n",
    "            'grocery_or_supermarket',\n",
    "            'supermarket',\n",
    "            'convenience_store'                 \n",
    "        ]\n",
    "        \n",
    "        # Add borough mappings to the PlacesDataCollector\n",
    "        self.borough_mappings = {\n",
    "            # Westminster postcodes\n",
    "            'W1': 'City of Westminster',\n",
    "            'W2': 'City of Westminster',\n",
    "            'W9': 'City of Westminster',\n",
    "            'SW1': 'City of Westminster',\n",
    "            'NW1': 'City of Westminster',\n",
    "            'NW8': 'City of Westminster',\n",
    "            # Specific Westminster postcodes\n",
    "            'W1A': 'City of Westminster',\n",
    "            'W1B': 'City of Westminster',\n",
    "            'W1C': 'City of Westminster',\n",
    "            'W1D': 'City of Westminster',\n",
    "            'W1F': 'City of Westminster',\n",
    "            'W1G': 'City of Westminster',\n",
    "            'W1H': 'City of Westminster',\n",
    "            'W1J': 'City of Westminster',\n",
    "            'W1K': 'City of Westminster',\n",
    "            'W1S': 'City of Westminster',\n",
    "            'W1T': 'City of Westminster',\n",
    "            'W1U': 'City of Westminster',\n",
    "            'W1W': 'City of Westminster',\n",
    "            # Kensington and Chelsea postcodes\n",
    "            'SW3': 'Kensington and Chelsea',\n",
    "            'SW5': 'Kensington and Chelsea',\n",
    "            'SW7': 'Kensington and Chelsea',\n",
    "            'SW10': 'Kensington and Chelsea',\n",
    "            'W8': 'Kensington and Chelsea',\n",
    "            'W10': 'Kensington and Chelsea',\n",
    "            'W11': 'Kensington and Chelsea',\n",
    "            'W14': 'Kensington and Chelsea'\n",
    "        }\n",
    "\n",
    "    def get_borough_from_postcode(self, postcode: str) -> str:\n",
    "        \"\"\"Determine borough from postcode using mapping.\"\"\"\n",
    "        if not postcode:\n",
    "            return 'Borough Unknown'\n",
    "            \n",
    "        # First try the full postcode prefix (before the space)\n",
    "        prefix = postcode.split(' ')[0] if ' ' in postcode else postcode\n",
    "        if prefix in self.borough_mappings:\n",
    "            return self.borough_mappings[prefix]\n",
    "            \n",
    "        # Then try the first part of the postcode\n",
    "        short_prefix = ''.join(filter(str.isalpha, prefix))\n",
    "        if short_prefix in self.borough_mappings:\n",
    "            return self.borough_mappings[short_prefix]\n",
    "            \n",
    "        return 'Borough Unknown'\n",
    "        \n",
    "    def get_location_details(self, postcode: str) -> tuple:\n",
    "        \"\"\"Get location details including coordinates and specific borough.\"\"\"\n",
    "        try:\n",
    "            geocode_result = self.gmaps.geocode(postcode)\n",
    "            if geocode_result:\n",
    "                location = geocode_result[0]['geometry']['location']\n",
    "                \n",
    "                # Use the new borough mapping function\n",
    "                borough = self.get_borough_from_postcode(postcode)\n",
    "                \n",
    "                return location['lat'], location['lng'], borough\n",
    "            return None, None, None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error geocoding postcode {postcode}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_place_details(self, place_id: str) -> dict:\n",
    "        \"\"\"Get detailed information about a specific place.\"\"\"\n",
    "        try:\n",
    "            time.sleep(2)  # Basic rate limiting\n",
    "            place_details = self.gmaps.place(place_id, fields=['name', 'geometry', 'type', 'opening_hours'])\n",
    "            return place_details.get('result', {})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching place details for {place_id}: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def simplify_store_type(self, store_type):\n",
    "        \"\"\"\n",
    "        Simplify store types into main categories:\n",
    "        - Supermarket for grocery/supermarket/convenience types\n",
    "        - Takeaway for everything else\n",
    "        \"\"\"\n",
    "        supermarket_types = ['grocery_or_supermarket', 'supermarket', 'convenience_store']\n",
    "        \n",
    "        if store_type in supermarket_types:\n",
    "            return 'Supermarket'\n",
    "        return 'Takeaway'\n",
    "\n",
    "    def format_time_HHMM(self, time_str):\n",
    "        \"\"\"Convert time from HHMM to HH:MM format.\"\"\"\n",
    "        if pd.isna(time_str) or time_str == '':\n",
    "            return None\n",
    "        return f\"{time_str[:2]}:{time_str[2:]}\"\n",
    "\n",
    "    def format_opening_hours(self, opening_hours: dict) -> list:\n",
    "        \"\"\"Format opening hours into required structure.\"\"\"\n",
    "        if not opening_hours or 'periods' not in opening_hours:\n",
    "            return []\n",
    "\n",
    "        days_data = []\n",
    "        for period in opening_hours['periods']:\n",
    "            if 'open' in period and 'close' in period:\n",
    "                days_data.append({\n",
    "                    'day': period['open']['day'],\n",
    "                    'opening_time': period['open']['time'],\n",
    "                    'closing_time': period['close']['time']\n",
    "                })\n",
    "        return days_data\n",
    "\n",
    "    def collect_places_data(self, postcode: str) -> pd.DataFrame:\n",
    "        \"\"\"Collect data for all food establishments in the given postcode.\"\"\"\n",
    "        print(f\"Starting data collection for {postcode}\")\n",
    "        \n",
    "        try:\n",
    "            lat, lng, borough = self.get_location_details(postcode)\n",
    "            if not lat or not lng:\n",
    "                raise ValueError(f\"Could not get coordinates for postcode {postcode}\")\n",
    "\n",
    "            all_places_data = []\n",
    "            # Track unique establishments by name AND location\n",
    "            collected_locations = set()  # Will store tuples of (name, lat, lng)\n",
    "            \n",
    "            for place_type in self.place_types:\n",
    "                print(f\"Searching for {place_type}...\")\n",
    "                \n",
    "                try:\n",
    "                    places_result = self.gmaps.places_nearby(\n",
    "                        location=(lat, lng),\n",
    "                        radius=1000,\n",
    "                        type=place_type\n",
    "                    )\n",
    "\n",
    "                    for place in places_result.get('results', []):\n",
    "                        place_name = place.get('name', '')\n",
    "                        place_lat = place.get('geometry', {}).get('location', {}).get('lat')\n",
    "                        place_lng = place.get('geometry', {}).get('location', {}).get('lng')\n",
    "                        \n",
    "                        # Create unique identifier using name and location\n",
    "                        location_identifier = (place_name, place_lat, place_lng)\n",
    "                        \n",
    "                        # Skip if we've already collected this exact establishment\n",
    "                        if location_identifier in collected_locations:\n",
    "                            continue\n",
    "                            \n",
    "                        collected_locations.add(location_identifier)\n",
    "                        print(f\"Found: {place_name} at ({place_lat}, {place_lng})\")\n",
    "                        place_details = self.get_place_details(place['place_id'])\n",
    "                        \n",
    "                        if not place_details:\n",
    "                            continue\n",
    "                            \n",
    "                        opening_hours = place_details.get('opening_hours', {})\n",
    "                        hours_data = self.format_opening_hours(opening_hours)\n",
    "                        \n",
    "                        for hours in hours_data:\n",
    "                            all_places_data.append({\n",
    "                                'Name': place_name,\n",
    "                                'Postcode': postcode,\n",
    "                                'Borough': borough,\n",
    "                                'Latitude': place_lat,\n",
    "                                'Longitude': place_lng,\n",
    "                                'Store_Type': self.simplify_store_type(place_type),\n",
    "                                'Day': hours.get('day', ''),\n",
    "                                'Opening_Hours': self.format_time_HHMM(hours.get('opening_time', '')),\n",
    "                                'Closing_Hours': self.format_time_HHMM(hours.get('closing_time', ''))\n",
    "                            })\n",
    "\n",
    "                    # Handle pagination if necessary\n",
    "                    while 'next_page_token' in places_result:\n",
    "                        time.sleep(2)\n",
    "                        places_result = self.gmaps.places_nearby(\n",
    "                            location=(lat, lng),\n",
    "                            radius=1000,\n",
    "                            type=place_type,\n",
    "                            page_token=places_result['next_page_token']\n",
    "                        )\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing {place_type}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(all_places_data)\n",
    "            \n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error collecting data for postcode {postcode}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2901194",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 3: Define Tableau Animation Function\n",
    "def prepare_data_for_tableau_animation(df):\n",
    "    \"\"\"Prepare the DataFrame for Tableau time-based animation, showing all hours.\"\"\"\n",
    "    \n",
    "    # Create a list to store expanded records\n",
    "    expanded_records = []\n",
    "    \n",
    "    # Base date\n",
    "    base_date = pd.Timestamp('2025-01-06')  # This is a Monday\n",
    "    \n",
    "    # Generate hourly intervals\n",
    "    time_intervals = pd.date_range(\"00:00:00\", \"23:00:00\", freq=\"1H\").time\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Convert opening and closing times to datetime.time objects\n",
    "        opening_time = pd.to_datetime(row['Opening_Hours'], format='%H:%M').time()\n",
    "        closing_time = pd.to_datetime(row['Closing_Hours'], format='%H:%M').time()\n",
    "        \n",
    "        # Get day number directly (it's already 0-6)\n",
    "        day_num = int(row['Day'])\n",
    "        \n",
    "        for interval in time_intervals:\n",
    "            # Handle overnight hours\n",
    "            if closing_time > opening_time:\n",
    "                # Normal hours (same day)\n",
    "                is_open = opening_time <= interval <= closing_time\n",
    "                time_date = pd.Timestamp.combine(base_date + pd.Timedelta(days=day_num), interval)\n",
    "            else:\n",
    "                # Overnight hours\n",
    "                if opening_time <= interval:\n",
    "                    # Current day after opening\n",
    "                    is_open = True\n",
    "                    time_date = pd.Timestamp.combine(base_date + pd.Timedelta(days=day_num), interval)\n",
    "                elif interval <= closing_time:\n",
    "                    # Next day until closing\n",
    "                    is_open = True\n",
    "                    time_date = pd.Timestamp.combine(base_date + pd.Timedelta(days=day_num + 1), interval)\n",
    "                else:\n",
    "                    # Outside of opening hours\n",
    "                    is_open = False\n",
    "                    time_date = pd.Timestamp.combine(base_date + pd.Timedelta(days=day_num), interval)\n",
    "                    # Also create next day early morning records\n",
    "                    if interval <= closing_time:\n",
    "                        expanded_records.append({\n",
    "                            'Name': row['Name'],\n",
    "                            'Postcode': row['Postcode'],\n",
    "                            'Borough': row['Borough'],\n",
    "                            'Latitude': row['Latitude'],\n",
    "                            'Longitude': row['Longitude'],\n",
    "                            'Store_Type': row['Store_Type'],\n",
    "                            'Day': row['Day'],\n",
    "                            'Opening_Hours': row['Opening_Hours'],\n",
    "                            'Closing_Hours': row['Closing_Hours'],\n",
    "                            'Time_Date': pd.Timestamp.combine(base_date + pd.Timedelta(days=day_num + 1), interval),\n",
    "                            'Is_Open': interval <= closing_time\n",
    "                        })\n",
    "            \n",
    "            expanded_records.append({\n",
    "                'Name': row['Name'],\n",
    "                'Postcode': row['Postcode'],\n",
    "                'Borough': row['Borough'],\n",
    "                'Latitude': row['Latitude'],\n",
    "                'Longitude': row['Longitude'],\n",
    "                'Store_Type': row['Store_Type'],\n",
    "                'Day': row['Day'],\n",
    "                'Opening_Hours': row['Opening_Hours'],\n",
    "                'Closing_Hours': row['Closing_Hours'],\n",
    "                'Time_Date': time_date,\n",
    "                'Is_Open': is_open\n",
    "            })\n",
    "    \n",
    "    # Create new DataFrame with expanded time intervals\n",
    "    df_expanded = pd.DataFrame(expanded_records)\n",
    "    \n",
    "    # Sort by establishment name and Time_Date\n",
    "    df_expanded = df_expanded.sort_values(['Name', 'Time_Date'])\n",
    "    \n",
    "    return df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bcd9dc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 4: PostcodeProcessor Class - Core Data Collection\n",
    "class PostcodeProcessor:\n",
    "    def __init__(self, timestamp, output_dir):\n",
    "        \"\"\"Initialize with complete postcodes for Westminster and Kensington & Chelsea.\"\"\"\n",
    "        # Westminster postcodes mapping - key areas with borough assignment\n",
    "        self.westminster_postcodes_map = {\n",
    "            'W1K 6TL': 'City of Westminster', 'W1S 1YQ': 'City of Westminster',\n",
    "            'W1B 5AH': 'City of Westminster', 'W1F 7TR': 'City of Westminster',\n",
    "            'W1T 7NN': 'City of Westminster', 'W1U 3QA': 'City of Westminster',\n",
    "            'W1G 8PY': 'City of Westminster', 'W1W 7JE': 'City of Westminster',\n",
    "            'W1H 7EY': 'City of Westminster', 'W2 1HY': 'City of Westminster',\n",
    "            'W2 2RL': 'City of Westminster', 'W2 4QJ': 'City of Westminster',\n",
    "            'W2 6BD': 'City of Westminster', 'NW8 7JN': 'City of Westminster',\n",
    "            'NW8 9AY': 'City of Westminster', 'NW8 0LH': 'City of Westminster',\n",
    "            'SW1A 1AA': 'City of Westminster', 'SW1E 5JL': 'City of Westminster',\n",
    "            'SW1H 9EA': 'City of Westminster', 'SW1P 3EU': 'City of Westminster',\n",
    "            'SW1V 1RG': 'City of Westminster', 'SW1W 0NY': 'City of Westminster',\n",
    "            'SW1X 7XL': 'City of Westminster', 'SW1Y 4AB': 'City of Westminster',\n",
    "            'W9 1AX': 'City of Westminster', 'W9 2AL': 'City of Westminster',\n",
    "            'W9 3DP': 'City of Westminster'\n",
    "        }\n",
    "        \n",
    "        # Kensington and Chelsea postcodes mapping\n",
    "        self.chelsea_postcodes_map = {\n",
    "            'SW3 1ER': 'Kensington and Chelsea', 'SW3 3DW': 'Kensington and Chelsea',\n",
    "            'SW3 5EL': 'Kensington and Chelsea', 'SW3 6RT': 'Kensington and Chelsea',\n",
    "            'SW5 9PR': 'Kensington and Chelsea', 'SW5 0SW': 'Kensington and Chelsea',\n",
    "            'SW5 9QP': 'Kensington and Chelsea', 'SW7 2AZ': 'Kensington and Chelsea',\n",
    "            'SW7 5BD': 'Kensington and Chelsea', 'SW7 4QL': 'Kensington and Chelsea',\n",
    "            'SW10 0XE': 'Kensington and Chelsea', 'SW10 9SU': 'Kensington and Chelsea',\n",
    "            'SW10 0AG': 'Kensington and Chelsea', 'W8 5ED': 'Kensington and Chelsea',\n",
    "            'W8 7LP': 'Kensington and Chelsea', 'W8 6DN': 'Kensington and Chelsea',\n",
    "            'W10 6TT': 'Kensington and Chelsea', 'W10 5XL': 'Kensington and Chelsea',\n",
    "            'W10 4AA': 'Kensington and Chelsea', 'W11 2BQ': 'Kensington and Chelsea',\n",
    "            'W11 3JQ': 'Kensington and Chelsea', 'W11 1QB': 'Kensington and Chelsea',\n",
    "            'W14 8TH': 'Kensington and Chelsea', 'W14 9JH': 'Kensington and Chelsea',\n",
    "            'W14 0RH': 'Kensington and Chelsea'\n",
    "        }\n",
    "        \n",
    "        # Combine for easy lookup\n",
    "        self.all_postcodes_map = {**self.westminster_postcodes_map, **self.chelsea_postcodes_map}\n",
    "        \n",
    "        # Extract lists for processing\n",
    "        self.westminster_postcodes = list(self.westminster_postcodes_map.keys())\n",
    "        self.chelsea_postcodes = list(self.chelsea_postcodes_map.keys())\n",
    "        \n",
    "        self.collector = PlacesDataCollector()\n",
    "        self.timestamp = timestamp\n",
    "        self.output_dir = output_dir\n",
    "        self.set_filenames()\n",
    "        \n",
    "        self.day_map = {\n",
    "            0: 'Sunday', 1: 'Monday', 2: 'Tuesday', 3: 'Wednesday',\n",
    "            4: 'Thursday', 5: 'Friday', 6: 'Saturday'\n",
    "        }\n",
    "    \n",
    "    def set_filenames(self, test_mode=False):\n",
    "        \"\"\"\n",
    "        Set filenames based on run mode.\n",
    "        \n",
    "        Args:\n",
    "            test_mode (bool): If True, adds 'test_' prefix to filenames\n",
    "        \"\"\"\n",
    "        mode_prefix = 'test_' if test_mode else ''\n",
    "        self.raw_filename = os.path.join(\n",
    "            self.output_dir,\n",
    "            f\"all_establishments_raw_{mode_prefix}{self.timestamp}.csv\"\n",
    "        )\n",
    "\n",
    "    def deduplicate_establishment_data(self, df):\n",
    "        \"\"\"\n",
    "        Remove duplicate establishments while preserving unique location data.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing establishment data\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Deduplicated data maintaining unique locations\n",
    "        \"\"\"\n",
    "        # Sort by Name, Lat, Long, and Day for consistent selection\n",
    "        df_sorted = df.sort_values(['Name', 'Latitude', 'Longitude', 'Day'])\n",
    "        \n",
    "        # Keep unique establishments based on name and location\n",
    "        unique_establishments = df_sorted.drop_duplicates(\n",
    "            subset=['Name', 'Latitude', 'Longitude', 'Day']\n",
    "        )\n",
    "        \n",
    "        return unique_establishments\n",
    "\n",
    "    def clean_boroughs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean borough data using predefined postcode mappings.\"\"\"\n",
    "        print(f\"Starting borough cleaning on {len(df)} rows\")\n",
    "        print(f\"Initial borough distribution:\")\n",
    "        print(df['Borough'].value_counts())\n",
    "        \n",
    "        def get_correct_borough(row):\n",
    "            postcode = row['Postcode']\n",
    "            current_borough = row['Borough']\n",
    "            if current_borough == 'Borough Unknown' and postcode in self.all_postcodes_map:\n",
    "                return self.all_postcodes_map[postcode]\n",
    "            # If still unknown, try using the collector's mapping\n",
    "            if current_borough == 'Borough Unknown':\n",
    "                return self.collector.get_borough_from_postcode(postcode)\n",
    "            return current_borough\n",
    "            \n",
    "        df['Borough'] = df.apply(get_correct_borough, axis=1)\n",
    "        \n",
    "        print(\"\\nAfter cleaning:\")\n",
    "        print(df['Borough'].value_counts())\n",
    "        print(f\"Final unique establishments: {len(df['Name'].unique())}\")\n",
    "        return df\n",
    "\n",
    "    def update_csv_data(self, new_data, filename):\n",
    "        \"\"\"\n",
    "        Update existing CSV with new batch data.\n",
    "        Handles first write and subsequent updates.\n",
    "        \n",
    "        Args:\n",
    "            new_data (pd.DataFrame): New batch of data to add\n",
    "            filename (str): Target CSV file\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Updated complete dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if os.path.exists(filename):\n",
    "                # Read existing data\n",
    "                existing_data = pd.read_csv(filename)\n",
    "                # Union with new data\n",
    "                updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "                # Remove any duplicates\n",
    "                updated_data = self.deduplicate_establishment_data(updated_data)\n",
    "            else:\n",
    "                updated_data = new_data\n",
    "                \n",
    "            # Save updated data\n",
    "            updated_data.to_csv(filename, index=False)\n",
    "            return updated_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating CSV {filename}: {str(e)}\")\n",
    "            new_data.to_csv(filename + '.backup', index=False)\n",
    "            return new_data\n",
    "        \n",
    "    def process_existing_data(self, input_filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Process existing raw data file instead of collecting new data.\"\"\"\n",
    "        try:\n",
    "            # Read existing data\n",
    "            raw_df = pd.read_csv(input_filename)\n",
    "            \n",
    "            # Clean boroughs\n",
    "            cleaned_df = self.clean_boroughs(raw_df)\n",
    "            \n",
    "            # Save to new file with timestamp\n",
    "            new_filename = os.path.join(\n",
    "                self.output_dir,\n",
    "                f\"all_establishments_raw_cleaned_{self.timestamp}.csv\"\n",
    "            )\n",
    "            cleaned_df.to_csv(new_filename, index=False)\n",
    "            print(f\"Cleaned data saved to: {new_filename}\")\n",
    "            \n",
    "            return cleaned_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing existing file {input_filename}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_postcodes(self, batch_size=2, delay_between_batches=600, test_mode=False, \n",
    "                        test_postcodes: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Process postcodes in batches and collect establishment data.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of postcodes to process in each batch\n",
    "            delay_between_batches (int): Delay in seconds between batches\n",
    "            test_mode (bool): If True, only process test postcodes\n",
    "            test_postcodes (List[str], optional): Specific postcodes to test\n",
    "        \"\"\"\n",
    "        self.set_filenames(test_mode)\n",
    "        \n",
    "        # Determine which postcodes to process\n",
    "        if test_mode:\n",
    "            if test_postcodes:\n",
    "                # Use provided test postcodes\n",
    "                all_postcodes = test_postcodes\n",
    "                print(\"\\nRUNNING IN TEST MODE - Processing specified test postcodes:\")\n",
    "                print(f\"Test postcodes: {', '.join(all_postcodes)}\")\n",
    "            else:\n",
    "                # Use default test selection: 3 from each borough\n",
    "                test_west = self.westminster_postcodes[:3]\n",
    "                test_chelsea = self.chelsea_postcodes[:3]\n",
    "                all_postcodes = test_west + test_chelsea\n",
    "                print(\"\\nRUNNING IN TEST MODE - Processing default test postcodes:\")\n",
    "                print(\"Westminster postcodes:\", ', '.join(test_west))\n",
    "                print(\"Chelsea postcodes:\", ', '.join(test_chelsea))\n",
    "        else:\n",
    "            # Process all postcodes\n",
    "            all_postcodes = self.westminster_postcodes + self.chelsea_postcodes\n",
    "        \n",
    "        batch_number = 1\n",
    "            \n",
    "        print(f\"Starting processing of {len(all_postcodes)} postcodes\")\n",
    "        print(f\"Processing in batches of {batch_size} with {delay_between_batches}s delay between batches\")\n",
    "        print(f\"Data will be saved to directory: {self.output_dir}\")\n",
    "        print(f\"Raw data file: {os.path.basename(self.raw_filename)}\")\n",
    "        \n",
    "        for i in range(0, len(all_postcodes), batch_size):\n",
    "                batch = all_postcodes[i:i + batch_size]\n",
    "                print(f\"\\nProcessing batch {batch_number}\")\n",
    "                print(f\"Postcodes: {', '.join(batch)}\")\n",
    "                batch_data = []\n",
    "                \n",
    "                for postcode in batch:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing {postcode}\")\n",
    "                        df = self.collector.collect_places_data(postcode)\n",
    "                        if not df.empty:\n",
    "                            batch_data.append(df)\n",
    "                            print(f\"Found {len(df)} establishments in {postcode}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing postcode {postcode}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if batch_data:\n",
    "                    # Combine batch data\n",
    "                    batch_df = pd.concat(batch_data, ignore_index=True)\n",
    "                    \n",
    "                    # Update raw data CSV\n",
    "                    print(f\"\\nUpdating raw data CSV...\")\n",
    "                    raw_data = self.update_csv_data(batch_df, self.raw_filename)\n",
    "                    \n",
    "                    # Print batch summary\n",
    "                    print(f\"\\nBatch {batch_number} Summary:\")\n",
    "                    print(f\"Processed postcodes: {', '.join(batch)}\")\n",
    "                    print(f\"New establishments: {len(batch_df)}\")\n",
    "                    print(f\"Total unique establishments: {len(raw_data['Name'].unique())}\")\n",
    "                    print(f\"Remaining postcodes: {len(all_postcodes) - (i + len(batch))}\")\n",
    "                    \n",
    "                    print(\"\\nCurrent data distribution:\")\n",
    "                    print(\"\\nBy borough:\")\n",
    "                    print(raw_data['Borough'].value_counts())\n",
    "                    print(\"\\nBy store type:\")\n",
    "                    print(raw_data['Store_Type'].value_counts())\n",
    "                \n",
    "                batch_number += 1\n",
    "                \n",
    "                if i + batch_size < len(all_postcodes):\n",
    "                    print(f\"\\nWaiting {delay_between_batches} seconds before next batch...\")\n",
    "                    time.sleep(delay_between_batches)\n",
    "            \n",
    "        print(\"\\nCollection complete!\")\n",
    "        if os.path.exists(self.raw_filename):\n",
    "            print(f\"Raw data available in: {self.raw_filename}\")\n",
    "            return pd.read_csv(self.raw_filename)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff21360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Tableau Data Preparation Class\n",
    "class TableauDataPreparator:\n",
    "    def __init__(self, timestamp, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize Tableau data preparation.\n",
    "        \n",
    "        Args:\n",
    "            timestamp (str): Timestamp for filenames\n",
    "            output_dir (str): Directory for output files\n",
    "        \"\"\"\n",
    "        self.timestamp = timestamp\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Set filenames with directory paths\n",
    "        self.tableau_filename = os.path.join(\n",
    "            self.output_dir,\n",
    "            f\"all_establishments_tableau_{timestamp}.csv\"\n",
    "        )\n",
    "        self.validation_filename = os.path.join(\n",
    "            self.output_dir,\n",
    "            f\"establishments_validation_{timestamp}.csv\"\n",
    "        )\n",
    "        \n",
    "        # Base date for time calculations\n",
    "        self.base_date = pd.Timestamp('2025-01-06')  # Monday\n",
    "        \n",
    "        # Day mapping for validation and reporting\n",
    "        self.day_map = {\n",
    "            0: 'Sunday',\n",
    "            1: 'Monday',\n",
    "            2: 'Tuesday',\n",
    "            3: 'Wednesday',\n",
    "            4: 'Thursday',\n",
    "            5: 'Friday',\n",
    "            6: 'Saturday'\n",
    "        }\n",
    "            \n",
    "    def prepare_data_for_tableau(self, raw_df):\n",
    "        \"\"\"Prepare raw data for Tableau visualization.\"\"\"\n",
    "        print(f\"Starting Tableau data preparation with {len(raw_df)} rows\")\n",
    "        expanded_records = []\n",
    "        time_intervals = pd.date_range(\"00:00:00\", \"23:00:00\", freq=\"h\").time  # Fixed deprecation warning\n",
    "        # Get unique establishments\n",
    "        unique_establishments = raw_df.drop_duplicates(subset=['Name', 'Latitude', 'Longitude'])\n",
    "        print(f\"Processing {len(unique_establishments)} unique establishments\")\n",
    "        \n",
    "        for idx, est in unique_establishments.iterrows():\n",
    "            if idx % 100 == 0:  # Progress update every 100 establishments\n",
    "                print(f\"Processing establishment {idx} of {len(unique_establishments)}\")\n",
    "                \n",
    "            # Get all records for this establishment\n",
    "            est_records = raw_df[\n",
    "                (raw_df['Name'] == est['Name']) & \n",
    "                (raw_df['Latitude'] == est['Latitude']) & \n",
    "                (raw_df['Longitude'] == est['Longitude'])\n",
    "            ]\n",
    "            \n",
    "            # Process each day (0-6)\n",
    "            for day in range(7):\n",
    "                # Get operating hours for current day and next day\n",
    "                day_records = est_records[est_records['Day'] == day]\n",
    "                next_day = (day + 1) % 7\n",
    "                next_day_records = est_records[est_records['Day'] == next_day]\n",
    "                \n",
    "                # Initialize time variables\n",
    "                opening_time = None\n",
    "                closing_time = None\n",
    "                max_closing_hour = None\n",
    "                is_overnight = False\n",
    "                \n",
    "                if not day_records.empty:\n",
    "                    try:\n",
    "                        # Get opening and closing times\n",
    "                        opening_time = pd.to_datetime(day_records.iloc[0]['Opening_Hours'], format='%H:%M').time()\n",
    "                        closing_time = pd.to_datetime(day_records.iloc[0]['Closing_Hours'], format='%H:%M').time()\n",
    "                        is_overnight = closing_time < opening_time\n",
    "                        \n",
    "                        if is_overnight and not next_day_records.empty:\n",
    "                            max_closing_hour = pd.to_datetime(next_day_records.iloc[0]['Closing_Hours'], format='%H:%M').time()\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        print(f\"Warning: Error processing times for {est['Name']} on day {day}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # Generate records for each hour\n",
    "                for interval in time_intervals:\n",
    "                    is_open = False\n",
    "                    actual_work_day = day\n",
    "                    \n",
    "                    if opening_time and closing_time:\n",
    "                        if not is_overnight:\n",
    "                            is_open = opening_time <= interval <= closing_time\n",
    "                        else:\n",
    "                            if opening_time <= interval:\n",
    "                                is_open = True\n",
    "                            elif max_closing_hour is not None and interval <= max_closing_hour:\n",
    "                                is_open = True\n",
    "                                actual_work_day = (day - 1) % 7\n",
    "                    \n",
    "                    calendar_datetime = pd.Timestamp.combine(\n",
    "                        self.base_date + pd.Timedelta(days=day), \n",
    "                        interval\n",
    "                    )\n",
    "                    \n",
    "                    actual_work_datetime = pd.Timestamp.combine(\n",
    "                        self.base_date + pd.Timedelta(days=actual_work_day), \n",
    "                        interval\n",
    "                    )\n",
    "                    \n",
    "                    expanded_records.append({\n",
    "                        'Name': est['Name'],\n",
    "                        'Postcode': est['Postcode'],\n",
    "                        'Borough': est['Borough'],\n",
    "                        'Latitude': est['Latitude'],\n",
    "                        'Longitude': est['Longitude'],\n",
    "                        'Store_Type': est['Store_Type'],\n",
    "                        'Day': day,\n",
    "                        'Actual_Day_of_Work': actual_work_day,\n",
    "                        'Opening_Hours': opening_time.strftime('%H:%M') if opening_time else None,\n",
    "                        'Closing_Hours': closing_time.strftime('%H:%M') if closing_time else None,\n",
    "                        'Time_Date': calendar_datetime,\n",
    "                        'Actual_Day_of_Work_DateTime': actual_work_datetime,\n",
    "                        'Is_Open': is_open\n",
    "                    })\n",
    "        \n",
    "        print(\"Creating DataFrame from expanded records...\")\n",
    "        df_expanded = pd.DataFrame(expanded_records)\n",
    "        if len(df_expanded) > 0:\n",
    "            df_expanded = df_expanded.sort_values(['Name', 'Latitude', 'Longitude', 'Time_Date'])\n",
    "            print(f\"Created DataFrame with {len(df_expanded)} rows for {len(df_expanded['Name'].unique())} establishments\")\n",
    "        else:\n",
    "            print(\"Warning: No records were created!\")\n",
    "        \n",
    "        return df_expanded\n",
    "    \n",
    "    \n",
    "    def update_tableau_data(self, raw_df):\n",
    "        \"\"\"\n",
    "        Update Tableau data file with new processed data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "        # Process new data\n",
    "            print(\"Processing data for Tableau...\")\n",
    "            print(f\"Input data contains {len(raw_df['Name'].unique())} unique establishments\")\n",
    "            \n",
    "            # Before processing\n",
    "            unique_locations = raw_df.groupby(['Name', 'Latitude', 'Longitude']).size().reset_index()\n",
    "            print(f\"Found {len(unique_locations)} unique location combinations\")\n",
    "            \n",
    "            new_tableau_data = self.prepare_data_for_tableau(raw_df)\n",
    "            \n",
    "            # After processing\n",
    "            unique_processed = new_tableau_data.groupby(['Name', 'Latitude', 'Longitude']).size().reset_index()\n",
    "            print(f\"Processed {len(unique_processed)} unique locations\")\n",
    "            \n",
    "            if os.path.exists(self.tableau_filename):\n",
    "                print(\"Updating existing Tableau data file...\")\n",
    "                existing_data = pd.read_csv(self.tableau_filename)\n",
    "                print(f\"Existing data has {len(existing_data['Name'].unique())} establishments\")\n",
    "                \n",
    "                # Union with new data\n",
    "                updated_data = pd.concat([existing_data, new_tableau_data], ignore_index=True)\n",
    "                \n",
    "                # Remove duplicates based on all identifying columns\n",
    "                updated_data = updated_data.drop_duplicates(\n",
    "                    subset=['Name', 'Latitude', 'Longitude', 'Day', 'Time_Date']\n",
    "                )\n",
    "                print(f\"After merging and deduplication: {len(updated_data['Name'].unique())} establishments\")\n",
    "            else:\n",
    "                print(\"Creating new Tableau data file...\")\n",
    "                updated_data = new_tableau_data\n",
    "            \n",
    "            # Save updated data\n",
    "            updated_data.to_csv(self.tableau_filename, index=False)\n",
    "            print(f\"Tableau data saved to: {self.tableau_filename}\")\n",
    "            print(f\"Final count: {len(updated_data['Name'].unique())} unique establishments\")\n",
    "            \n",
    "            # Create validation results\n",
    "            validation_results = self.validate_data(updated_data)\n",
    "            validation_results.to_csv(self.validation_filename, index=False)\n",
    "            print(f\"Validation results saved to: {self.validation_filename}\")\n",
    "            \n",
    "            return updated_data\n",
    "    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating Tableau data: {str(e)}\")\n",
    "            print(f\"Error during Tableau data update: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "\n",
    "    def validate_data(self, tableau_df):\n",
    "        \"\"\"\n",
    "        Validate the Tableau data for completeness and consistency.\n",
    "        \n",
    "        Args:\n",
    "            tableau_df (pd.DataFrame): Processed Tableau data\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Validation results\n",
    "        \"\"\"\n",
    "        validation_results = []\n",
    "        unique_establishments = tableau_df.drop_duplicates(subset=['Name', 'Latitude', 'Longitude'])\n",
    "        \n",
    "        for _, est in unique_establishments.iterrows():\n",
    "            # Get all records for this establishment\n",
    "            est_records = tableau_df[\n",
    "                (tableau_df['Name'] == est['Name']) & \n",
    "                (tableau_df['Latitude'] == est['Latitude']) & \n",
    "                (tableau_df['Longitude'] == est['Longitude'])\n",
    "            ]\n",
    "            \n",
    "            # Check for complete coverage\n",
    "            hours_per_day = est_records.groupby('Day').size()\n",
    "            is_complete = (len(hours_per_day) == 7) and all(count == 24 for count in hours_per_day)\n",
    "            \n",
    "            # Check for overnight operations\n",
    "            has_overnight = any(\n",
    "                est_records['Day'] != est_records['Actual_Day_of_Work']\n",
    "            )\n",
    "            \n",
    "            validation_results.append({\n",
    "                'Name': est['Name'],\n",
    "                'Location': f\"({est['Latitude']}, {est['Longitude']})\",\n",
    "                'Store_Type': est['Store_Type'],\n",
    "                'Borough': est['Borough'],\n",
    "                'Total_Hours': len(est_records),\n",
    "                'Complete_Coverage': is_complete,\n",
    "                'Has_Overnight_Operations': has_overnight,\n",
    "                'Hours_Per_Day': dict(hours_per_day)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a739e19",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection with continuous CSV updates:\n",
      "Mode: TEST - Selected postcodes only\n",
      "Test postcodes: ['W1K 6TL', 'SW1A 1AA', 'W1S 1YQ', 'SW1X 7XL', 'SW1W 0NY', 'SW1P 3EU']\n",
      "- Processing 2 postcodes at a time\n",
      "- 10 minute break between batches\n",
      "\n",
      "Output Directory: deliveroo_data_20250109_010753\n",
      "\n",
      "RUNNING IN TEST MODE - Processing specified test postcodes:\n",
      "Test postcodes: W1K 6TL, SW1A 1AA, W1S 1YQ, SW1X 7XL, SW1W 0NY, SW1P 3EU\n",
      "Starting processing of 6 postcodes\n",
      "Processing in batches of 2 with 600s delay between batches\n",
      "Data will be saved to directory: deliveroo_data_20250109_010753\n",
      "Raw data file: all_establishments_raw_test_20250109_010753.csv\n",
      "\n",
      "Processing batch 1\n",
      "Postcodes: W1K 6TL, SW1A 1AA\n",
      "\n",
      "Processing W1K 6TL\n",
      "Starting data collection for W1K 6TL\n",
      "Searching for restaurant...\n",
      "Found: Claridge's at (51.5125404, -0.1478631)\n",
      "Found: The Chesterfield Mayfair at (51.5076899, -0.1471358)\n",
      "Found: The Mandeville Hotel at (51.51647850000001, -0.1509183)\n",
      "Found: Durrants Hotel at (51.51800189999999, -0.1532301)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- 10 minute break between batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput Directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     raw_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_postcodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelay_between_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_postcodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_postcodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# If data was collected successfully\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 216\u001b[0m, in \u001b[0;36mPostcodeProcessor.process_postcodes\u001b[0;34m(self, batch_size, delay_between_batches, test_mode, test_postcodes)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpostcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_places_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    218\u001b[0m         batch_data\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "Cell \u001b[0;32mIn[3], line 169\u001b[0m, in \u001b[0;36mPlacesDataCollector.collect_places_data\u001b[0;34m(self, postcode)\u001b[0m\n\u001b[1;32m    167\u001b[0m collected_locations\u001b[38;5;241m.\u001b[39madd(location_identifier)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplace_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplace_lat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplace_lng\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m place_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_place_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplace\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplace_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m place_details:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m, in \u001b[0;36mPlacesDataCollector.get_place_details\u001b[0;34m(self, place_id)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get detailed information about a specific place.\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Basic rate limiting\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     place_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgmaps\u001b[38;5;241m.\u001b[39mplace(place_id, fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopening_hours\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m place_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 6: Execute Data Collection and Preparation Process\n",
    "# Initialize timestamp and output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = f\"deliveroo_data_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set parameters\n",
    "test_mode = True  # Set to False for full run\n",
    "input_file = None  # Set to filename if processing existing data to skip API collection process\n",
    "test_postcodes = [\n",
    "    'W1K 6TL', 'SW1A 1AA', 'W1S 1YQ',  # Westminster\n",
    "    'SW1X 7XL', 'SW1W 0NY', 'SW1P 3EU'     # Chelsea\n",
    "]\n",
    "\n",
    "# Initialize processors\n",
    "processor = PostcodeProcessor(timestamp, output_dir)\n",
    "tableau_preparator = TableauDataPreparator(timestamp, output_dir)\n",
    "\n",
    "if input_file:\n",
    "    print(f\"Processing existing file: {input_file}\")\n",
    "    raw_df = processor.process_existing_data(input_file)\n",
    "else:\n",
    "    print(\"Starting collection with continuous CSV updates:\")\n",
    "    print(f\"Mode: {'TEST - Selected postcodes only' if test_mode else 'FULL RUN'}\")\n",
    "    if test_mode:\n",
    "        print(\"Test postcodes:\", test_postcodes)\n",
    "    print(\"- Processing 2 postcodes at a time\")\n",
    "    print(\"- 10 minute break between batches\")\n",
    "    print(f\"\\nOutput Directory: {output_dir}\")\n",
    "    \n",
    "    raw_df = processor.process_postcodes(\n",
    "        batch_size=2, \n",
    "        delay_between_batches=600,\n",
    "        test_mode=test_mode,\n",
    "        test_postcodes=test_postcodes if test_mode else None\n",
    "    )\n",
    "\n",
    "# If data was collected successfully\n",
    "if raw_df is not None:\n",
    "    print(\"\\nProcessing final Tableau data...\")\n",
    "    tableau_df = tableau_preparator.update_tableau_data(raw_df)\n",
    "    \n",
    "    print(\"\\nFinal Collection Summary:\")\n",
    "    print(f\"Total unique establishments: {len(raw_df['Name'].unique())}\")\n",
    "    \n",
    "    print(\"\\nEstablishments by borough:\")\n",
    "    display(raw_df['Borough'].value_counts())\n",
    "    \n",
    "    print(\"\\nEstablishments by type:\")\n",
    "    display(raw_df['Store_Type'].value_counts())\n",
    "    \n",
    "    print(\"\\nData time ranges:\")\n",
    "    print(\"Calendar time range:\")\n",
    "    print(f\"Start: {tableau_df['Time_Date'].min()}\")\n",
    "    print(f\"End: {tableau_df['Time_Date'].max()}\")\n",
    "    print(\"\\nActual work day time range:\")\n",
    "    print(f\"Start: {tableau_df['Actual_Day_of_Work_DateTime'].min()}\")\n",
    "    print(f\"End: {tableau_df['Actual_Day_of_Work_DateTime'].max()}\")\n",
    "    \n",
    "    # Show overnight operations analysis\n",
    "    overnight_ops = tableau_df[\n",
    "        tableau_df['Day'] != tableau_df['Actual_Day_of_Work']\n",
    "    ]['Name'].unique()\n",
    "    \n",
    "    if len(overnight_ops) > 0:\n",
    "        print(f\"\\nFound {len(overnight_ops)} establishments with overnight operations\")\n",
    "        print(\"Sample of overnight establishments:\")\n",
    "        sample_df = tableau_df[\n",
    "            (tableau_df['Name'].isin(overnight_ops[:3])) & \n",
    "            (tableau_df['Is_Open'])\n",
    "        ][['Name', 'Day', 'Actual_Day_of_Work', 'Time_Date', \n",
    "            'Actual_Day_of_Work_DateTime', 'Opening_Hours', 'Closing_Hours']]\n",
    "        display(sample_df.head(10))\n",
    "\n",
    "print(\"\\nScript execution complete!\")\n",
    "print(f\"All output files are in directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9bb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "lenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
